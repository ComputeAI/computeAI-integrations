version: "3.3"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    depends_on:
      - demoDatasetDownload
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./dataset:/dataset
    environment:
      - CLUSTER_NAME=computeai-hadoop-sandbox
      - HDFS_CONF_dfs_replication=1
    env_file:
      - ./hadoop/hadoop.env
    build:
      context: .
      dockerfile: ./hadoop/namenode/Dockerfile-namenode

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - demoDatasetDownload
      - namenode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CLUSTER_NAME=computeai-hadoop-sandbox
      - HDFS_CONF_dfs_replication=1
    env_file:
      - ./hadoop/hadoop.env
    build:
      context: .
      dockerfile: ./hadoop/datanode/Dockerfile-datanode

  notebook:
    container_name: jupyterNotebook
    depends_on:
      - jitsuOrchestrator
    labels:
      description: Intel Python 2 using Jupyter Notebooks
      name: jupyter notebook 
    ports:
      - "8888:8888"
    volumes:
      - ~/notebooks:/home/notebooks
    build: ./jupyter 
    command: jupyter notebook --NotebookApp.token='infinite-scale'

  jitsuServer:
    image: jitsuai/compute_ai:jitsu-server-781e40ef64a35aaf43b4c01ee2ef49357f534c43
    container_name: jitsuServer
    depends_on:
      - demoDatasetDownload
    ulimits:
      nofile: 1048576
      memlock: -1
      core: 0
    expose:
      - 50051
    ports:
      - "50051:50051"
    environment:
      - JITSU_SWAP_PATH=${JITSU_SWAP_PATH}
      - JITSU_SERVER_BUILD=${JITSU_SERVER_BUILD}
      - CONTAINER_NAME=jitsuServer
      - TERMINFO=/usr/share/terminfo # To be able to run jitsu_top.py from within the container
    cap_add:
      - SYS_PTRACE
    security_opt:
      - seccomp=unconfined
    volumes:
      - ./jitsuServerConfig/jitsuServer/jitsuServerConfig.json:/opt/jitsu/jitsuServer.json
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/jitsu/jitsu-warehouse:/tmp/jitsu-warehouse
      - /opt/jitsu/logs/:/opt/jitsu/logs/
      - ./dataset:/dataset

  jitsuOrchestrator:
    image: jitsuai/compute_ai:jdbc-server-781e40ef64a35aaf43b4c01ee2ef49357f534c43
    container_name: jitsuOrchestrator
    depends_on:
      - jitsuServer
      - demoDatasetDownload
      - namenode
    environment:
      - GRPC_SERVER_HOST=jitsuServer
      - GRPC_SERVER_PORT=50051
      - CONTAINER_NAME=jitsuOrchestrator
      - JDBC_SERVER_TRANSPORT_MODE=TCP
      - JDBC_AUTH=PAM
      - HIVE_SERVER2_IDLE_SESSION_TIMEOUT_SECONDS=${HIVE_SERVER2_IDLE_SESSION_TIMEOUT_SECONDS}
      - HIVE_SERVER2_SESSION_CHECK_INTERVAL_SECONDS=${HIVE_SERVER2_SESSION_CHECK_INTERVAL_SECONDS}
      - JEC_INSTANCE_CHECK_INTERVAL_SECONDS=${JEC_INSTANCE_CHECK_INTERVAL_SECONDS}
      - JDBC_AUTH=PASSWORD
      - JDBC_PASSWORD=${JDBC_PASSWORD}
      - JDBC_PORT=10000
      - DASHBOARD_PORT=8080
      - WAREHOUSE_LOCATION=${JEC_WAREHOUSE_LOCATION}
      - METASTORE_LOCATION=${JEC_METASTORE_LOCATION}
      - JEC_UNIQUE_TAG_VALUE=${JEC_UNIQUE_TAG_VALUE}
      - SPARK_LOG_DIR=/opt/jitsu/logs
      - ENABLE_CACHING=${ENABLE_CACHING}
      - CACHE_CLUSTER_ID=${CACHE_CLUSTER_ID}
      - CACHE_EXPIRY_TIME=${CACHE_EXPIRY_TIME}
      - MAX_CACHE_RESULT_SET_SIZE_MB=${MAX_CACHE_RESULT_SET_SIZE_MB}
      - JITSU_CUSTOMER_ID=${JITSU_CUSTOMER_ID}
      - JITSU_CUSTOMER_ATTRIBUTE=${JITSU_CUSTOMER_ATTRIBUTE}
      - JEC_ORCHESTRATOR_INSTANCE_TYPE=${JEC_ORCHESTRATOR_INSTANCE_TYPE}
      - MAX_SESSIONS_PER_NODE=${MAX_SESSIONS_PER_NODE}
      - PERCENT_MEMROY_TO_ALLOCATE_TO_SPARK_DRIVER=90
      - GET_SESSION_TIMEOUT_SECONDS=${GET_SESSION_TIMEOUT_SECONDS}
      - HDFS_WAREHOUSE_LOCATION=hdfs://namenode:9000
      - CATALOG_TYPE=hive_hdfs
    expose:
      - 10000
      - 8080
    ports:
      - "10000:10000"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /opt/jitsu/jitsu-metastore:/tmp/jitsu-metastore
      - /opt/jitsu/jitsu-warehouse:/tmp/jitsu-warehouse
      - /opt/jitsu/logs/:/opt/jitsu/logs/
      - ./dataset:/dataset

  demoDatasetDownload:
    image: node:14-alpine
    entrypoint: ["/bin/sh","-c"]
    volumes:
      - ./dataset:/dataset/
    command:
      - |
        echo "Install AWS CLI"
        apk update
        apk add python3 py3-pip groff less openssl ca-certificates
        pip3 install awscli
        aws --version
        echo "Download TPCH & TPCDS SF1 dataset"
        mkdir -p /dataset/tpch/sf1
        aws s3 cp s3://jitsuai-public-datasets/tpch/sf1/ /dataset/tpch/sf1 --recursive --exclude "*" --include "*.parquet"
        mkdir -p /dataset/tpcds/sf1
        aws s3 cp s3://jitsuai-public-datasets/tpcds/sf1/ /dataset/tpcds/sf1 --recursive --exclude "*" --include "*.parquet"

volumes:
  hadoop_namenode:
  hadoop_datanode:
